{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae6e00d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c5b4c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Vivek\n",
      "[nltk_data]     Ghodmare\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Vivek\n",
      "[nltk_data]     Ghodmare\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK stopwords data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e30bd102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_article(url):\n",
    "    # Send a GET request to the webpage\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the webpage\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the article text\n",
    "    article_text = ''\n",
    "    article_text_elements = soup.find_all('p')  # Assuming the article text is contained within <p> tags\n",
    "    for element in article_text_elements:\n",
    "        article_text += element.get_text() + '\\n'\n",
    "\n",
    "    return article_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f8ca143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extracted from article 1 saved to file.\n",
      "Text extracted from article 2 saved to file.\n",
      "Text extracted from article 3 saved to file.\n",
      "Text extracted from article 4 saved to file.\n",
      "Text extracted from article 5 saved to file.\n",
      "Text extracted from article 6 saved to file.\n",
      "Text extracted from article 7 saved to file.\n",
      "Text extracted from article 8 saved to file.\n",
      "Text extracted from article 9 saved to file.\n",
      "Text extracted from article 10 saved to file.\n",
      "Text extracted from article 11 saved to file.\n",
      "Text extracted from article 12 saved to file.\n",
      "Text extracted from article 13 saved to file.\n",
      "Text extracted from article 14 saved to file.\n",
      "Text extracted from article 15 saved to file.\n",
      "Text extracted from article 16 saved to file.\n",
      "Text extracted from article 17 saved to file.\n",
      "Text extracted from article 18 saved to file.\n",
      "Text extracted from article 19 saved to file.\n",
      "Text extracted from article 20 saved to file.\n",
      "Text extracted from article 21 saved to file.\n",
      "Text extracted from article 22 saved to file.\n",
      "Text extracted from article 23 saved to file.\n",
      "Text extracted from article 24 saved to file.\n",
      "Text extracted from article 25 saved to file.\n",
      "Text extracted from article 26 saved to file.\n",
      "Text extracted from article 27 saved to file.\n",
      "Text extracted from article 28 saved to file.\n",
      "Text extracted from article 29 saved to file.\n",
      "Text extracted from article 30 saved to file.\n",
      "Text extracted from article 31 saved to file.\n",
      "Text extracted from article 32 saved to file.\n",
      "Text extracted from article 33 saved to file.\n",
      "Text extracted from article 34 saved to file.\n",
      "Text extracted from article 35 saved to file.\n",
      "Text extracted from article 36 saved to file.\n",
      "Text extracted from article 37 saved to file.\n",
      "Text extracted from article 38 saved to file.\n",
      "Text extracted from article 39 saved to file.\n",
      "Text extracted from article 40 saved to file.\n",
      "Text extracted from article 41 saved to file.\n",
      "Text extracted from article 42 saved to file.\n",
      "Text extracted from article 43 saved to file.\n",
      "Text extracted from article 44 saved to file.\n",
      "Text extracted from article 45 saved to file.\n",
      "Text extracted from article 46 saved to file.\n",
      "Text extracted from article 47 saved to file.\n",
      "Text extracted from article 48 saved to file.\n",
      "Text extracted from article 49 saved to file.\n",
      "Text extracted from article 50 saved to file.\n",
      "Text extracted from article 51 saved to file.\n",
      "Text extracted from article 52 saved to file.\n",
      "Text extracted from article 53 saved to file.\n",
      "Text extracted from article 54 saved to file.\n",
      "Text extracted from article 55 saved to file.\n",
      "Text extracted from article 56 saved to file.\n",
      "Text extracted from article 57 saved to file.\n",
      "Text extracted from article 58 saved to file.\n",
      "Text extracted from article 59 saved to file.\n",
      "Text extracted from article 60 saved to file.\n",
      "Text extracted from article 61 saved to file.\n",
      "Text extracted from article 62 saved to file.\n",
      "Text extracted from article 63 saved to file.\n",
      "Text extracted from article 64 saved to file.\n",
      "Text extracted from article 65 saved to file.\n",
      "Text extracted from article 66 saved to file.\n",
      "Text extracted from article 67 saved to file.\n",
      "Text extracted from article 68 saved to file.\n",
      "Text extracted from article 69 saved to file.\n",
      "Text extracted from article 70 saved to file.\n",
      "Text extracted from article 71 saved to file.\n",
      "Text extracted from article 72 saved to file.\n",
      "Text extracted from article 73 saved to file.\n",
      "Text extracted from article 74 saved to file.\n",
      "Text extracted from article 75 saved to file.\n",
      "Text extracted from article 76 saved to file.\n",
      "Text extracted from article 77 saved to file.\n",
      "Text extracted from article 78 saved to file.\n",
      "Text extracted from article 79 saved to file.\n",
      "Text extracted from article 80 saved to file.\n",
      "Text extracted from article 81 saved to file.\n",
      "Text extracted from article 82 saved to file.\n",
      "Text extracted from article 83 saved to file.\n",
      "Text extracted from article 84 saved to file.\n",
      "Text extracted from article 85 saved to file.\n",
      "Text extracted from article 86 saved to file.\n",
      "Text extracted from article 87 saved to file.\n",
      "Text extracted from article 88 saved to file.\n",
      "Text extracted from article 89 saved to file.\n",
      "Text extracted from article 90 saved to file.\n",
      "Text extracted from article 91 saved to file.\n",
      "Text extracted from article 92 saved to file.\n",
      "Text extracted from article 93 saved to file.\n",
      "Text extracted from article 94 saved to file.\n",
      "Text extracted from article 95 saved to file.\n",
      "Text extracted from article 96 saved to file.\n",
      "Text extracted from article 97 saved to file.\n",
      "Text extracted from article 98 saved to file.\n",
      "Text extracted from article 99 saved to file.\n",
      "Text extracted from article 100 saved to file.\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords function\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    return ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
    "\n",
    "# Read URLs from Excel file\n",
    "df = pd.read_excel('../../DSML28/Blackcofferr/Input_Blackcofffer.xlsx') \n",
    "urls = df['URL'].tolist() \n",
    "\n",
    "# To Iterate through each URL, extract text, and save to a text file\n",
    "for index, url in enumerate(urls):\n",
    "    try:\n",
    "        article_text = extract_text_from_article(url)\n",
    "        \n",
    "        # Remove stopwords from the text\n",
    "        filtered_text = remove_stopwords(article_text)\n",
    "        \n",
    "        # Save extracted text to a text file\n",
    "        with open(f'article_{index+1}.txt', 'w', encoding='utf-8') as file:\n",
    "            file.write(filtered_text)\n",
    "        \n",
    "        print(f\"Text extracted from article {index+1} saved to file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract text from article {index+1}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09327e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsml28_env1]",
   "language": "python",
   "name": "conda-env-dsml28_env1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
