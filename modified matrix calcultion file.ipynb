{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad65c3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Vivek\n",
      "[nltk_data]     Ghodmare\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Vivek\n",
      "[nltk_data]     Ghodmare\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Vivek\n",
      "[nltk_data]     Ghodmare\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output written to output.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download NLTK resources if not already downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize VADER Sentiment Intensity Analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to clean the text\n",
    "def clean_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    tokens = [word.lower() for word in tokens if word.lower() not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Function to calculate average sentence length\n",
    "def avg_sentence_length(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return len(word_tokenize(text)) / len(sentences)\n",
    "\n",
    "# Function to calculate percentage of complex words\n",
    "def percentage_complex_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    complex_words = [word for word in words if len(word) > 2]  # Words with more than 2 characters\n",
    "    return (len(complex_words) / len(words)) * 100\n",
    "\n",
    "# Function to calculate fog index\n",
    "def fog_index(text):\n",
    "    avg_sent_length = avg_sentence_length(text)\n",
    "    pct_complex_words = percentage_complex_words(text)\n",
    "    return 0.4 * (avg_sent_length + pct_complex_words)\n",
    "\n",
    "# Function to calculate average number of words per sentence\n",
    "def avg_words_per_sentence(text):\n",
    "    words = word_tokenize(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    return len(words) / len(sentences)\n",
    "\n",
    "# Function to count complex words\n",
    "def complex_word_count(text):\n",
    "    words = word_tokenize(text)\n",
    "    complex_words = [word for word in words if len(word) > 2]  # Words with more than 2 characters\n",
    "    return len(complex_words)\n",
    "\n",
    "# Function to count total words\n",
    "def word_count(text):\n",
    "    words = word_tokenize(text)\n",
    "    return len(words)\n",
    "\n",
    "# Function to count syllables in a word\n",
    "def count_syllables(word):\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    vowels = 'aeiouy'\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith('e'):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count = 1\n",
    "    return count\n",
    "\n",
    "# Function to calculate syllables per word\n",
    "def syllables_per_word(text):\n",
    "    words = word_tokenize(text)\n",
    "    syllable_count = sum(count_syllables(word) for word in words)\n",
    "    return syllable_count / len(words)\n",
    "\n",
    "# Function to count personal pronouns\n",
    "def personal_pronouns(text):\n",
    "    pronouns = re.findall(r'\\b(?:I|we|my|ours|us)\\b', text, flags=re.IGNORECASE)\n",
    "    return len(pronouns)\n",
    "\n",
    "# Function to calculate average word length\n",
    "def avg_word_length(text):\n",
    "    words = word_tokenize(text)\n",
    "    total_characters = sum(len(word) for word in words)\n",
    "    return total_characters / len(words)\n",
    "\n",
    "# Process each text file and generate output\n",
    "def process_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Clean the text\n",
    "    cleaned_text = ' '.join(clean_text(text))\n",
    "\n",
    "    # Perform sentiment analysis using VADER\n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    avg_sent_length = avg_sentence_length(cleaned_text)\n",
    "    pct_complex_words = percentage_complex_words(cleaned_text)\n",
    "    fog_idx = fog_index(cleaned_text)\n",
    "    avg_words_per_sent = avg_words_per_sentence(cleaned_text)\n",
    "    complex_count = complex_word_count(cleaned_text)\n",
    "    total_words = word_count(cleaned_text)\n",
    "    syllables_per_word_count = syllables_per_word(cleaned_text)\n",
    "    personal_pronouns_count = personal_pronouns(cleaned_text)\n",
    "    avg_word_length_val = avg_word_length(cleaned_text)\n",
    "\n",
    "    # Calculate Polarity Score\n",
    "    positive_score = sentiment_scores['pos']\n",
    "    negative_score = sentiment_scores['neg']\n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "\n",
    "    # Calculate Subjectivity Score\n",
    "    subjectivity_score = (positive_score + negative_score) / (total_words + 0.000001)\n",
    "\n",
    "    return {\n",
    "        'Positive Score': positive_score,\n",
    "        'Negative Score': negative_score,\n",
    "        'Polarity Score': polarity_score,\n",
    "        'Subjectivity Score': subjectivity_score,\n",
    "        'Average Sentence Length': avg_sent_length,\n",
    "        'Percentage of Complex Words': pct_complex_words,\n",
    "        'Fog Index': fog_idx,\n",
    "        'Average Number of Words Per Sentence': avg_words_per_sent,\n",
    "        'Complex Word Count': complex_count,\n",
    "        'Word Count': total_words,\n",
    "        'Syllables Per Word': syllables_per_word_count,\n",
    "        'Personal Pronouns Count': personal_pronouns_count,\n",
    "        'Average Word Length': avg_word_length_val\n",
    "    }\n",
    "\n",
    "# Process all text files in a directory\n",
    "def process_text_files_in_directory(directory):\n",
    "    output_data = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            output = process_text_file(file_path)\n",
    "            output_data.append(output)\n",
    "    return output_data\n",
    "\n",
    "# Directory containing text files\n",
    "input_directory = '../../DSML28/Blackcofferr/Extracted_Text_Files/'\n",
    "\n",
    "# Process all text files in the directory\n",
    "output_data = process_text_files_in_directory(input_directory)\n",
    "\n",
    "# Create DataFrame from output data\n",
    "output_df = pd.DataFrame(output_data)\n",
    "\n",
    "# Read original Excel file containing 'URL_ID' and 'URL' columns\n",
    "original_excel_file = '../../DSML28/Blackcofferr/Output Data Structure - Blackcoffer.xlsx'\n",
    "original_df = pd.read_excel(original_excel_file)\n",
    "\n",
    "# Merge original DataFrame with output DataFrame\n",
    "merged_df = pd.concat([original_df[['URL_ID', 'URL']], output_df], axis=1)\n",
    "\n",
    "# Output Excel file\n",
    "output_excel_file = 'output.xlsx'\n",
    "\n",
    "# Write merged DataFrame to Excel\n",
    "merged_df.to_excel(output_excel_file, index=False)\n",
    "\n",
    "print(\"Output written to\", output_excel_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75720e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsml28_env1]",
   "language": "python",
   "name": "conda-env-dsml28_env1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
